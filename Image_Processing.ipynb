{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "reading all libraries and packages that we need\n",
    "'''\n",
    "import os\n",
    "import sys\n",
    "import mrc\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "import SimpleITK as sitk\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage.segmentation import mark_boundaries,clear_border, watershed\n",
    "from skimage.measure import regionprops, label\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.morphology import closing, square, remove_small_objects, binary_erosion, disk, binary_dilation, convex_hull_image\n",
    "#from skimage.color import label2rgb\n",
    "from skimage.filters import  threshold_otsu, threshold_triangle, gaussian, threshold_local\n",
    "from skimage.draw import line, polygon\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_and_label_dapi(seg_dapi, debris_size, erosion_radius):\n",
    "    ''' Segments the dapi image.\n",
    "    Ipnuts:\n",
    "    \n",
    "    seg_dapi: 2-dimensional numpy array (segmented image array)\n",
    "    debris_size: size of the debris to be removed (in pixels)\n",
    "    erosion_radius: radius (in pixels) of the disk to be used for binary ersion to separate connected nuclei\n",
    "    \n",
    "    Outputs:\n",
    "    nuclear_mask: segmented nuclei image\n",
    "    nuclear_labels: labled nuclei\n",
    "    \n",
    "    '''\n",
    "    # Step 1: Remove small debris from the segmented image using the specified debris size threshold\n",
    "    seg_dapi_1 = remove_small_objects(seg_dapi, debris_size)\n",
    "\n",
    "    # Step 2: Erode the image slightly using a disk-shaped structuring element (disk size 2) to disconnect close objects\n",
    "    seg_dapi_2 = binary_erosion(seg_dapi_1, disk(2))\n",
    "    \n",
    "    # Step 3: Fill any holes in the eroded image to create solid nuclei regions\n",
    "    nuclear_mask = ndi.binary_fill_holes(seg_dapi_2)\n",
    "    \n",
    "    # Step 4: Further erode the nuclear mask using a disk with the specified erosion radius\n",
    "    # This helps in separating nuclei that are too close together\n",
    "    eroded_mask = binary_erosion(nuclear_mask, disk(erosion_radius))\n",
    "\n",
    "    # Step 5: Perform distance transform to compute the Euclidean distance to the nearest background pixel\n",
    "    distance = ndi.distance_transform_edt(eroded_mask)\n",
    "\n",
    "    # Step 6: Find local maxima in the distance-transformed image, which will serve as markers for the watershed\n",
    "    local_maxi = peak_local_max(distance, indices=False, footprint=np.ones((1, 1)),\n",
    "                                labels=nuclear_mask)\n",
    "\n",
    "    # Step 7: Label the local maxima to create markers\n",
    "    markers = ndi.label(local_maxi)[0]\n",
    "\n",
    "    # Step 8: Apply the watershed algorithm using the negative distance transform and the markers\n",
    "    # This will segment the nuclei by separating connected regions based on the distance map\n",
    "    nuclear_labels = watershed(-distance, markers, mask=nuclear_mask, connectivity=2)\n",
    "    \n",
    "    # Return the segmented mask and the labeled nuclei\n",
    "    return nuclear_mask, nuclear_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_and_label_dapi(dapi_image, debris_size, min_area_to_keep_cell, erosion_radius, block_size = 251):\n",
    "    ''' Segments the dapi image.\n",
    "    Ipnuts:\n",
    "    \n",
    "    dapi_image: 2-dimensional numpy array (image array)\n",
    "    debris_size: size of the debris to be removed (in pixels)\n",
    "    erosion_radius: radius (in pixels) of the disk to be used for binary ersion to separate connected nuclei\n",
    "    \n",
    "    Outputs:\n",
    "    nuclear_mask: segmented nuclei image\n",
    "    nuclear_labels: labled nuclei\n",
    "    \n",
    "    '''\n",
    "    # Step 1: Perform local adaptive thresholding using a block size to binarize the DAPI image\n",
    "    # Adaptive thresholding adjusts the threshold for small regions, useful for uneven illumination\n",
    "    adaptive_thresh = threshold_local(dapi_image, block_size = block_size)\n",
    "\n",
    "    # Step 2: Create a binary mask where the DAPI image is greater than the adaptive threshold\n",
    "    seg_dapi = dapi_image > adaptive_thresh\n",
    "    \n",
    "\n",
    "    # Step 3: Fill any small holes in the binary mask to create solid nuclei regions\n",
    "    nuclear_mask = ndi.binary_fill_holes(seg_dapi)\n",
    "    \n",
    "    # Step 4: Remove small debris from the segmented image based on the debris size threshold\n",
    "    seg_dapi = remove_small_objects(seg_dapi, debris_size)\n",
    "\n",
    "    \n",
    "    # Step 5: Remove any small nuclei that are below the minimum area threshold\n",
    "    nuclear_mask = remove_small_objects(nuclear_mask, min_area_to_keep_cell)\n",
    "    \n",
    "    # Step 6: Erode the image using a disk-shaped structuring element (with the specified erosion radius)\n",
    "    # This helps in separating nuclei that are close together\n",
    "    eroded_mask = binary_erosion(nuclear_mask, disk(erosion_radius))\n",
    "\n",
    "    # Step 7: Compute the Euclidean distance transform to measure the distance to the nearest background pixel\n",
    "    distance = ndi.distance_transform_edt(eroded_mask)\n",
    "\n",
    "    # Step 8: Identify local maxima in the distance-transformed image, which will be used as markers for watershed\n",
    "    local_maxi = peak_local_max(distance, indices=False, footprint=np.ones((1, 1)),\n",
    "                                labels=nuclear_mask)\n",
    "\n",
    "    # Step 9: Label the local maxima to generate markers for the watershed algorithm\n",
    "    markers = ndi.label(local_maxi)[0]\n",
    "\n",
    "    # Step 10: Apply the watershed algorithm to segment the nuclei based on the distance transform\n",
    "    # This separates connected regions using the markers as starting points\n",
    "    nuclear_labels = watershed(-distance, markers, mask=nuclear_mask, connectivity=2)\n",
    "\n",
    "    # Return the final segmented mask and labeled nuclei\n",
    "    return nuclear_mask, nuclear_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_big_cells(labeled_img, area_thresh, cir_thresh):\n",
    "    ''' Determining the package of the cells as a big cell.\n",
    "    Ipnuts:\n",
    "    \n",
    "    labeled_img: nuclei label from the labling function\n",
    "    area_thresh: maximum size of a nucleus (in pixels) to be considered as a single nucleus\n",
    "    cir_thresh: circularity threshold to see whether it is a single nucleus or multiple nuclei\n",
    "    \n",
    "    Outputs:\n",
    "    big_cells: label of the cells that are satistfying the area AND circularity threshold\n",
    "    '''\n",
    "    # Lambda function to calculate the circularity of a nucleus\n",
    "    # Circularity is calculated as (4 * Ï€ * Area) / Perimeter^2\n",
    "    # This gives a value between 0 and 1, with values closer to 1 indicating more circular shapes\n",
    "    \n",
    "    circ = lambda r: (4 * np.pi * r.area) / (r.perimeter * r.perimeter)\n",
    "\n",
    "    # List comprehension to find nuclei that satisfy both area and circularity thresholds\n",
    "    big_cells = [(prop.label, prop.area, circ(prop)) # For each region, store the label, area, and circularity\n",
    "                 for prop in regionprops(labeled_img) # Iterate through each labeled region (nucleus)\n",
    "                 if prop.area > area_thresh and circ(prop) < cir_thresh] # Apply the area and circularity filters\n",
    "\n",
    "    # Return the list of big cells that meet the criteria\n",
    "    return big_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cells(label_rem, image):\n",
    "    ''' Splitting a package of the cells\n",
    "    Ipnuts:\n",
    "    \n",
    "    labeled_rem: label of the object that should be splitted\n",
    "    image: image: 2D labeled image\n",
    "    \n",
    "    Outputs:\n",
    "    labels_rem: label of the cells that are going to split\n",
    "    \n",
    "    '''\n",
    "    # Initialize arrays to store relabeled and temporary images\n",
    "    relabels, tmp_img = np.zeros_like(image), np.zeros_like(image)\n",
    "\n",
    "    # Initialize lists to store points and coordinates of centroids\n",
    "    pts = []\n",
    "    row_cords =[]\n",
    "    col_cords = []\n",
    "\n",
    "    # Step 1: Loop through each region in the labeled object to extract its centroid coordinates\n",
    "    for prop in regionprops(label_rem):\n",
    "        x,y = np.int16(np.round(prop.centroid)) # Get the centroid coordinates (rounded to integer)\n",
    "        row_cords.append(x) # Store row coordinates\n",
    "        col_cords.append(y) # Store column coordinates\n",
    "        pts.extend([x,y]) # Add the centroid coordinates to the points list\n",
    "\n",
    "    # Step 2: If there are more than 2 regions, draw a polygon around the centroids\n",
    "    if len(regionprops(label_rem))>2:\n",
    "        rr, cc = polygon(row_cords, col_cords) # Create a polygon connecting the centroids\n",
    "        \n",
    "    else:\n",
    "        # Otherwise, draw a straight line between two centroids\n",
    "        rr, cc = line(pts[0], pts[1], pts[2], pts[3])\n",
    "\n",
    "    # Step 3: Mark the region inside the polygon/line in the temporary image\n",
    "    tmp_img[rr,cc] =1\n",
    "    \n",
    "    # Step 4: Dilate the binary line/polygon to cover more area\n",
    "    tmp_img = binary_dilation(tmp_img, disk(10))\n",
    "\n",
    "    # Step 5: Compute the convex hull of the dilated image to create a filled area\n",
    "    tmp_img = convex_hull_image(tmp_img)\n",
    "\n",
    "    # Step 6: Create a mask for the regions that are not covered by the convex hull\n",
    "    split_img = np.logical_and(image, np.logical_not(tmp_img))\n",
    "\n",
    "    # Step 7: Perform distance transform to compute the distance of each pixel to the nearest background\n",
    "    distance_rem = ndi.distance_transform_edt(split_img)\n",
    "\n",
    "    # Step 8: Find local maxima in the distance-transformed image, which will act as markers for watershed\n",
    "    local_maxi_rem = peak_local_max(distance_rem, indices=False, footprint=np.ones((1, 1)),\n",
    "                                labels=image)\n",
    "\n",
    "    # Step 9: Label the local maxima as markers for watershed segmentation\n",
    "    markers_rem = label(local_maxi_rem, connectivity=2)\n",
    "\n",
    "    # Step 10: Apply the watershed algorithm to split connected regions based on the distance map\n",
    "    labels_rem = watershed(-distance_rem, markers_rem, mask=image, connectivity=2)\n",
    "\n",
    "    # Return the relabeled image after splitting\n",
    "    return labels_rem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_final_labels_after_splitting_big_objects(nuclear_labels, big_cells, \n",
    "                                                    ero_rad, min_rem_area, min_area_to_keep_cell):\n",
    "    '''\n",
    "    Inputs:\n",
    "    nuclear_labels: 2D numpy labeled dapi image\n",
    "    big_cells: label of the big objects\n",
    "    ero_rad: radius of the disk to be used for eroading the region (convex hull - roi)\n",
    "    min_rem_area: removing any object less than of this size (in pixels)\n",
    "    min_area_to_keep_cell: minimum size of an acceptable nucleus (in mpixels)\n",
    "    \n",
    "    Outputs:\n",
    "    final_label: final label of nuecleus after removing small objects and splitting big packages\n",
    "    '''\n",
    "\n",
    "    # Initialize an array to hold the relabeled image after splitting large objects\n",
    "    relabels = np.zeros_like(nuclear_labels)\n",
    "\n",
    "    # Step 1: Loop through each big cell that needs to be split\n",
    "    for ii, area, cir in big_cells:\n",
    "        image = nuclear_labels==ii # Create a binary mask for the current big cell\n",
    "\n",
    "        # Step 2: Generate the convex hull of the current cell to form a tight boundary around it\n",
    "        chull = convex_hull_image(image)\n",
    "\n",
    "        # Step 3: Identify the region outside the original cell but within the convex hull\n",
    "        rem = np.logical_and(chull, np.logical_not(image))\n",
    "\n",
    "        # Step 4: Erode this external region to shrink it slightly and separate connected objects\n",
    "        eroded_rem = binary_erosion(rem, disk(ero_rad))\n",
    "\n",
    "        # Step 5: Remove small objects from the eroded region based on the minimum remaining area threshold\n",
    "        eroded_rem = remove_small_objects(eroded_rem, min_rem_area)\n",
    "\n",
    "        # Step 6: Label the remaining eroded region to identify individual components\n",
    "        label_rem = label(eroded_rem)\n",
    "\n",
    "        # Step 7: If there are two or more connected components in the labeled region, split them\n",
    "        if np.max(label_rem) >= 2:\n",
    "            # Update the relabeled image with the split cells from the large object\n",
    "            relabels = label(relabels + split_cells(label_rem, image))## split cells\n",
    "\n",
    "    # Step 8: Initialize an array to hold the newly assigned labels after splitting\n",
    "    assigned_relabels = np.zeros_like(relabels)\n",
    "\n",
    "    # Step 9: Assign new labels to the split regions, ensuring no overlap with existing labels\n",
    "    for p in regionprops(relabels):\n",
    "        if p.label > 0:\n",
    "            # Assign a new label to each split region that doesn't overlap with existing labels\n",
    "            assigned_relabels[np.where(relabels==p.label)] = np.max(nuclear_labels) + p.label\n",
    "\n",
    "    # Step 10: Add the original nuclear labels and the newly assigned relabeled regions\n",
    "    label_img = label(nuclear_labels + assigned_relabels)\n",
    "    \n",
    "    # Step 11: Remove small objects from the final labeled image based on the minimum area threshold for a valid nucleus\n",
    "    label_img = remove_small_objects(label_img, min_area_to_keep_cell)\n",
    "\n",
    "    # Final relabeled image after splitting and filtering\n",
    "    final_labels = label_img\n",
    "    return final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(dir_name):\n",
    "    # Check if the directory does not exist\n",
    "    if not os.path.exists(dir_name):\n",
    "            # If the directory does not exist, create it\n",
    "            os.makedirs(dir_name)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = r\"D:\\12-January21\\GREB1\\FVWAE2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpaths = [os.path.join(root_path,f) for f in os.listdir(root_path) if f.endswith('.dv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Parameters for Image Segmentation and Processing\n",
    "sigma = 1 # Standard deviation for Gaussian filtering, typically used for smoothing or noise reduction\n",
    "erosion_radius = 31 # Radius (in pixels) of the disk used for morphological erosion; larger values lead to more aggressive erosion\n",
    "debris_size = 100 # Minimum size (in pixels) for debris to be removed during preprocessing; any object smaller than this will be considered debris\n",
    "min_area_to_keep_cell = 3000 # Minimum size (in pixels) of an acceptable nucleus; nuclei smaller than this size will be filtered out\n",
    "block_size = 651 # Block size for adaptive thresholding; defines the local neighborhood size used to calculate the threshold for segmentation\n",
    "dilation_radius = 100 # Radius (in pixels) for morphological dilation; larger values expand the object boundaries during post-processing\n",
    "area_thresh =20000 # Maximum area (in pixels) for a nucleus to be considered a single cell; objects larger than this will likely be split\n",
    "cir_thresh = 0.70 # Circularity threshold; objects with circularity below this value will be considered for splitting, as they are likely not single cells\n",
    "ero_rad = 5 # Radius (in pixels) of the disk used for eroding the convex hull region; smaller values make more subtle changes\n",
    "min_rem_area = 2 # Minimum remaining area (in pixels) after erosion for the region to be kept; smaller objects are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory name that includes specific parameters in the name\n",
    "seg_dir = os.path.join(root_path, 'Segmentation_erosion_radius_'+ str(erosion_radius)\n",
    "                       + '_min_area_' + str(min_area_to_keep_cell) +'_area_thresh_' + str(area_thresh))\n",
    "\n",
    "# Check if the directory already exists\n",
    "if not os.path.exists(seg_dir):\n",
    "    # If the directory does not exist, create it\n",
    "    os.makedirs(seg_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for the directory to store raw images\n",
    "raw_img_dir = os.path.join(os.path.dirname(seg_dir), 'raw')\n",
    "# Define the path for the directory to store raw images with top and bottom cropped\n",
    "raw_img_dir_leave_top_bottom = os.path.join(os.path.dirname(seg_dir), 'raw_projected')\n",
    "# Define the path for the directory to store segmented DAPI images\n",
    "seg_dapi_dir = os.path.join(os.path.dirname(seg_dir), 'seg_dapi')\n",
    "# List of directories to create if they don't already exist\n",
    "for dir_name in [raw_img_dir, raw_img_dir_leave_top_bottom, seg_dapi_dir]:\n",
    "    # Check if the directory does not exist\n",
    "    if not os.path.exists(dir_name):\n",
    "        # Create the directory\n",
    "        os.makedirs(dir_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to store features\n",
    "features = pd.DataFrame()\n",
    "# Iterate over each file path in the list 'fpaths'\n",
    "for fpath in fpaths:\n",
    "    # Extract the file name from the path\n",
    "    fname = os.path.basename(fpath)\n",
    "    # Create a main file name by joining parts of the file name with underscores\n",
    "    main_fname = ('_').join(fname.split('_')[0:])\n",
    "    # Check if 'wash' is present in the file name\n",
    "    if 'wash' in fname:\n",
    "        # Remove the file extension from the file name\n",
    "        fname_noext = os.path.splitext(fname)[0]\n",
    "        # Split the file name into components\n",
    "        year, mon_date,  gene,_,_,_,time,field, _, _ =fname_noext.split('_')\n",
    "        # Append a new row to the DataFrame with extracted features\n",
    "        features = features.append([{'filename': main_fname,\n",
    "                                         'filepath': fpath,\n",
    "                                         'date': ('').join([year,mon_date]),\n",
    "                                         'gene': gene,\n",
    "                                         'time': time,\n",
    "                                         'field' : field,\n",
    "                                          },])\n",
    "    else:\n",
    "        # Remove the file extension from the file name\n",
    "        fname_noext = os.path.splitext(fname)[0]\n",
    "        # Split the file name into components\n",
    "        year, mon_date, gene,_,field,_,_ =fname_noext.split('_')\n",
    "        # Append a new row to the DataFrame with extracted features\n",
    "        features = features.append([{'filename': main_fname,\n",
    "                                         'filepath': fpath,\n",
    "                                         'date': ('').join([year,mon_date]),\n",
    "                                         'gene': gene,\n",
    "                                         'time': '0',\n",
    "                                         'field' : field,\n",
    "                                          },])\n",
    "# Reset the DataFrame index after appending rows\n",
    "features = features.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_projection(path,df):\n",
    "        \"\"\"\n",
    "    Process an image file by performing the following steps:\n",
    "    1. Read the image and compute projections for different channels.\n",
    "    2. Save the projections to specified directories.\n",
    "    3. Perform segmentation and labeling on the DAPI channel.\n",
    "    4. Identify and segment large cells and refine labels.\n",
    "    5. Mark boundaries of nuclei and save the results.\n",
    "\n",
    "    Parameters:\n",
    "    - path: str\n",
    "      The file path to the image to be processed.\n",
    "    - df: pandas.DataFrame\n",
    "      A DataFrame containing metadata about the images. Used to filter for the current image.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Extract the base name of the file and change the extension to '.tif'\n",
    "    fname = os.path.splitext(os.path.basename(path))[0] + '.tif'\n",
    "\n",
    "    # Filter the DataFrame to get the row corresponding to the given file path\n",
    "    df = df[df['filepath'] == path]\n",
    "\n",
    "    # Read the image file into an array\n",
    "    img_array = mrc.imread(path)\n",
    "\n",
    "    # Get the dimension size of the image\n",
    "    DIM=img_array.shape[1]\n",
    "\n",
    "    # Compute the maximum projection along the z-axis for each channel and save the results\n",
    "    # Projection for DAPI channel (all slices)\n",
    "    dapi_image = np.max(img_array[0], axis=0)\n",
    "    io.imsave(os.path.join(raw_img_dir, 'dapi_' + fname), dapi_image)\n",
    "    \n",
    "    # Projection for DAPI channel (excluding top and bottom slices)\n",
    "    dapi_image = np.max(img_array[0, 2:DIM-1], axis=0)\n",
    "    io.imsave(os.path.join(raw_img_dir_leave_top_bottom, 'dapi_' + fname), dapi_image)\n",
    "    \n",
    "    # Projection for Exon channel (all slices)\n",
    "    exon_image = np.max(img_array[1], axis=0)\n",
    "    io.imsave(os.path.join(raw_img_dir,'exon_' + fname), exon_image)\n",
    "    \n",
    "    # Projection for Exon channel (excluding top and bottom slices)\n",
    "    exon_image = np.max(img_array[1, 2:DIM-1], axis=0)\n",
    "    io.imsave(os.path.join(raw_img_dir_leave_top_bottom,'exon_' + fname), exon_image)\n",
    "    \n",
    "    # Projection for Intron channel (all slices)\n",
    "    intron_image = np.max(img_array[2], axis=0)\n",
    "    io.imsave(os.path.join(raw_img_dir,'intron_' + fname), intron_image)\n",
    "    \n",
    "    # Projection for Intron channel (excluding top and bottom slices)\n",
    "    intron_image = np.max(img_array[2, 2:DIM-1], axis=0)\n",
    "    io.imsave(os.path.join(raw_img_dir_leave_top_bottom,'intron_' + fname), intron_image)\n",
    "    \n",
    "    # Segment and label nuclei in the DAPI image\n",
    "    nuclear_mask, nuclear_labels = segment_and_label_dapi(dapi_image, debris_size, \n",
    "                                                          min_area_to_keep_cell,\n",
    "                                                          erosion_radius,\n",
    "                                                         block_size = block_size)\n",
    "\n",
    "    # Identify and segment big cells in the nuclear labels\n",
    "    big_cells = obtain_big_cells(nuclear_labels, area_thresh, cir_thresh)\n",
    "\n",
    "    # Obtain final labels after splitting big cells and additional processing\n",
    "    final_labels = obtain_final_labels_after_splitting_big_objects(nuclear_labels, big_cells, ero_rad, \n",
    "                                                min_rem_area, min_area_to_keep_cell)\n",
    "    # Clear labels of nuclei touching the border\n",
    "    non_border_labels = clear_border(final_labels)\n",
    "    # Create a mask for nuclei\n",
    "    nuc_mask = final_labels > 0\n",
    "    # Mark boundaries of nuclei in the DAPI image\n",
    "    marked_dapi = mark_boundaries(dapi_image, non_border_labels, color=(1, 1, 1), outline_color=(1, 1, 1))\n",
    "    # Save the marked DAPI image with boundaries highlighted\n",
    "    io.imsave(os.path.join(seg_dapi_dir, 'dapi_' + fname), np.uint8(marked_dapi*255))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelize the execution of the image_projection function across multiple files\n",
    "# - n_jobs=8: Specifies that 8 parallel jobs (or threads) will be used.\n",
    "# - delayed: Wraps the image_projection function to delay its execution until it's ready to be parallelized.\n",
    "# - features['filepath'].unique(): Retrieves the unique file paths from the 'features' DataFrame, so that the function is applied to each unique path.\n",
    "\n",
    "project=Parallel(n_jobs=8)(delayed(image_projection)(path,features) for path in features['filepath'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of full file paths for all TIFF files in the directory `raw_img_dir_leave_top_bottom`\n",
    "# Generate the full path by joining the directory path with each file name\n",
    "# Iterate over all files in the specified directory\n",
    "# Include only files that end with the '.tif' extension\n",
    "\n",
    "newfpaths = [os.path.join(raw_img_dir_leave_top_bottom,f) for f in os.listdir(raw_img_dir_leave_top_bottom) if f.endswith('.tif')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segmenting DAPI channel\n",
    "# Define the alpha parameter for sharpening\n",
    "\n",
    "alpha = 40\n",
    "# Iterate over each file path in the list of new TIFF files\n",
    "for fpath in newfpaths:\n",
    "    # Process only the files that have 'dapi' in their file path\n",
    "    if 'dapi' in fpath:\n",
    "        \n",
    "        # Read the DAPI image from the file path\n",
    "        dapi_image = io.imread(fpath)\n",
    "\n",
    "        # Create a local variable for convenience\n",
    "        f = dapi_image\n",
    "        \n",
    "        # Apply Gaussian blur to the image with a sigma of 3\n",
    "        blurred_f = ndi.gaussian_filter(f, 3)\n",
    "\n",
    "        # Apply a second Gaussian blur with a smaller sigma of 1\n",
    "        filter_blurred_f = ndi.gaussian_filter(blurred_f, 1)\n",
    "\n",
    "        # Sharpen the image by subtracting the blurred image from the original and scaling\n",
    "        sharpened = blurred_f + alpha * (blurred_f - filter_blurred_f)\n",
    "\n",
    "        # Apply a binary threshold to the DAPI image to fill holes\n",
    "        filled_seg_dapi = ndi.binary_fill_holes(dapi_image > (0.6*threshold_otsu(dapi_image)))\n",
    "\n",
    "        # Compute the local adaptive threshold for the sharpened image\n",
    "        adaptive_thresh = threshold_local(sharpened, block_size = block_size)\n",
    "\n",
    "        # Apply the local adaptive threshold to the sharpened image and fill holes\n",
    "        temp_seg_2 = ndi.binary_fill_holes (sharpened > adaptive_thresh)\n",
    "        \n",
    "        # Combine the binary masks from the DAPI thresholding and adaptive thresholding\n",
    "        seg_dapi = np.logical_and(filled_seg_dapi, temp_seg_2)\n",
    "\n",
    "        # Extract the base name of the file\n",
    "        fname = os.path.basename(fpath)\n",
    "\n",
    "        # Save the segmented DAPI image to the specified directory\n",
    "        io.imsave(os.path.join(seg_dapi_dir, 'seg_' + fname), np.uint8(255*seg_dapi))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to store new features\n",
    "newfeatures = pd.DataFrame()\n",
    "\n",
    "# Iterate over each file path in the list of new TIFF files\n",
    "for fpath in newfpaths:\n",
    "    # Extract the base name of the file\n",
    "    fname = os.path.basename(fpath)\n",
    "     # Create a main file name by joining parts of the file name with underscores\n",
    "    main_fname = ('_').join(fname.split('_')[1:])\n",
    "    # Process files containing 'wash' in their name\n",
    "    if 'wash' in fname:\n",
    "        # Remove the file extension from the file name\n",
    "        fname_noext = os.path.splitext(fname)[0]\n",
    "        # Split the file name into components\n",
    "        wavelength, year, mon_date, _,_,_,_,time,field, _, _ =fname_noext.split('_')\n",
    "        # Append the extracted features to the DataFrame\n",
    "        newfeatures = newfeatures.append([{'filename': main_fname,\n",
    "                                         'filepath': fpath,\n",
    "                                         'date': ('').join([year,mon_date]),\n",
    "                                         'time': time,\n",
    "                                         'field' : field,\n",
    "                                     'wavelength' : wavelength,\n",
    "                                          },])\n",
    "    else:\n",
    "        # Remove the file extension from the file name\n",
    "        fname_noext = os.path.splitext(fname)[0]\n",
    "         # Split the file name into components\n",
    "        wavelength, year, mon_date, _,_, field, _, _ =fname_noext.split('_')\n",
    "        # Append the extracted features to the DataFrame with a default time value\n",
    "        newfeatures = newfeatures.append([{'filename': main_fname,\n",
    "                                         'filepath': fpath,\n",
    "                                         'date': ('').join([year,mon_date]),\n",
    "                                         'time': '0',\n",
    "                                         'field' : field,\n",
    "                                     'wavelength' : wavelength,\n",
    "                                          },])\n",
    "# Reset the DataFrame index after appending rows\n",
    "newfeatures = newfeatures.reset_index(drop=True)\n",
    "# Convert the 'field' column to integer type\n",
    "newfeatures = newfeatures.astype({\"field\": int})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to map time strings to numerical values\n",
    "t = {'15min': 15, '30min':30, '45min': 45, '60min': 60, '75min': 75, '90min':90,'0':0}\n",
    "# Map the 'time' column in the DataFrame to the corresponding numerical values using the dictionary\n",
    "newfeatures[\"time\"] = newfeatures[\"time\"].map(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a directory path for storing segmentation results, incorporating key parameters into the folder name\n",
    "seg_dir = os.path.join(root_path,  'Segmentation_erosion_radius_'+ str(erosion_radius)\n",
    "                       + '_min_area_' + str(min_area_to_keep_cell) +'_area_thresh_' + str(area_thresh))\n",
    "\n",
    "# Check if the directory already exists, and if not, create it\n",
    "if not os.path.exists(seg_dir):\n",
    "    os.makedirs(seg_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_segmentation(filename,feat):\n",
    "        \"\"\"\n",
    "    This function segments and analyzes images based on different channels (DAPI, exon, intron).\n",
    "    It performs segmentation on nuclei (DAPI), exonic spots, and intronic spots, and then checks \n",
    "    for colocalization between exonic and intronic spots to identify nascent mRNA. The results are\n",
    "    saved into a summary dictionary, and finally saved to a pickle file.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): The name of the file to process.\n",
    "        feat (DataFrame): DataFrame containing features including file paths, time, field, and wavelength.\n",
    "\n",
    "    Returns:\n",
    "        None. (Saves the segmentation summary to a pickle file.)\n",
    "    \"\"\"\n",
    "    summary = {}\n",
    "\n",
    "    # Filter the DataFrame for the specific file being processed\n",
    "    df = feat[feat['filename'] == filename]\n",
    "    t=df.iloc[0]['time']#str(df.iloc[0]['time']) + \"_\" + df.iloc[0]['treatment']\n",
    "    f=df.iloc[0]['field']\n",
    "    \n",
    "    # Initialize summary dictionary for this time and field\n",
    "    summary['time' +str(t)+'field'+str(f)]={}\n",
    "    \n",
    "    # Loop over each channel (wavelength) in the DataFrame for the current file\n",
    "    for channel in df['wavelength']:\n",
    "        sdf = df[df['wavelength']==channel]\n",
    "        fname = os.path.basename(list(sdf['filepath'])[0])\n",
    "        \n",
    "        ## Segmenting the nuclei in the DAPI channel\n",
    "        if channel == 'dapi':\n",
    "            seg_name = 'seg_' + fname\n",
    "            seg_dapi =  io.imread(os.path.join(seg_dapi_dir, seg_name))\n",
    "            \n",
    "            # Prune and label the DAPI channel to get nuclear labels\n",
    "            nuclear_mask, nuclear_labels = prune_and_label_dapi(seg_dapi, debris_size, erosion_radius)\n",
    "\n",
    "            # Obtain big cells that may need splitting if they contain more than one nucleus\n",
    "            big_cells = obtain_big_cells(nuclear_labels, area_thresh, cir_thresh)\n",
    "\n",
    "            # Split large objects and obtain final nuclear labels\n",
    "            final_labels_1 = obtain_final_labels_after_splitting_big_objects(nuclear_labels, big_cells, ero_rad, \n",
    "                                                                min_rem_area, min_area_to_keep_cell)\n",
    "\n",
    "            nuc_mask = final_labels_1\n",
    "\n",
    "            # Dilate the nuclear mask slightly to ensure full coverage\n",
    "            temp_img = np.logical_or(binary_dilation(nuc_mask, disk(2)), binary_fill_holes(seg_dapi))\n",
    "\n",
    "            # Use the watershed algorithm to segment cells based on nuclear labels\n",
    "            final_labels = watershed(temp_img, markers=final_labels_1, mask=temp_img)\n",
    "\n",
    "            # Remove labels of nuclei that touch the border of the image\n",
    "            non_border_labels = clear_border(final_labels)\n",
    "            \n",
    "\n",
    "        ## Segmenting the exonic spots in the exon channel\n",
    "        elif channel == 'exon':\n",
    "            exon_image =  io.imread(os.path.join(fpath, fname))\n",
    "            seg_exon = exon_image > (threshold_otsu(exon_image))\n",
    "            \n",
    "        \n",
    "        ## Segmenting the intronic spots in the intron channel\n",
    "        elif channel == 'intron':\n",
    "            intron_image = sitk.ReadImage(os.path.join(fpath, fname))\n",
    "            gaussian_blur = sitk.SmoothingRecursiveGaussianImageFilter()\n",
    "            gaussian_blur.SetSigma ( float ( sigma ) )\n",
    "            blur_intron = gaussian_blur.Execute ( intron_image )\n",
    "\n",
    "            # Apply Maximum Entropy Thresholding for segmentation\n",
    "            max_entropy_filter = sitk.MaximumEntropyThresholdImageFilter()\n",
    "            max_entropy_filter.SetInsideValue(0)\n",
    "            max_entropy_filter.SetOutsideValue(1)\n",
    "            seg = max_entropy_filter.Execute(blur_intron)\n",
    "            seg_intron = sitk.GetArrayFromImage(seg)\n",
    "\n",
    "            # Determine threshold for final segmentation\n",
    "            blur_intron_img = sitk.GetArrayFromImage(blur_intron)\n",
    "            intron_threshold = np.min(blur_intron_img[np.where(seg_intron != 0)])\n",
    "            seg_intron = blur_intron_img > intron_threshold\n",
    "            \n",
    "        else:\n",
    "            print('Different than dapi, exon or intron image found!')\n",
    "            \n",
    "    ## Saving the segmentation of all three channels as a combined image                \n",
    "    comb_img = np.zeros((seg_exon.shape[0], seg_exon.shape[0],3)) # Initialize combined image\n",
    "    comb_img[:,:,0] = seg_intron # Red channel for intron\n",
    "    comb_img[:,:,1] = seg_exon # Green channel for exon\n",
    "    comb_img[:,:,2] = nuc_mask # Blue channel for DAPI (nuclei)\n",
    "\n",
    "    # Mark boundaries of nuclei and save the combined image\n",
    "    marked_img = mark_boundaries(comb_img, non_border_labels, color=(1, 1, 1), outline_color=(1, 1, 1))\n",
    "    io.imsave(os.path.join(seg_dir, 'combined_' + fname), np.uint8(marked_img*255))\n",
    "    \n",
    "    \n",
    "    ## Label the segmented spots in all channels    \n",
    "    green=label(seg_exon) # Label exonic spots\n",
    "    red=label(seg_intron) # Label intronic spots\n",
    "    blue=label(non_border_labels) # Label DAPI (nuclei)\n",
    "        \n",
    "\n",
    "\n",
    "    ### Extract measurements for each nucleus\n",
    "    \n",
    "    ## First, for each nucleus within the DAPI channel we find the exonic spots that are inside that nucleus\n",
    "\n",
    "    for nuc in regionprops(blue):\n",
    "        nuc_id=nuc.label # Get the nucleus ID\n",
    "        summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]={}\n",
    "        # Initialize lists to store spot data for each nucleus\n",
    "        summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['intron']=[]\n",
    "        summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['exon']=[]\n",
    "        summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['colocspot']=[]\n",
    "        summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['Ncolocspot']=[]\n",
    "        summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['nascent']=[]\n",
    "        summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['Nnascent']=[]\n",
    "\n",
    "        # Identify exonic spots that are inside the nucleus\n",
    "        for exon in regionprops(green,intensity_image=exon_image):\n",
    "            exon_id=exon.label\n",
    "            if (((exon.coords[:, None] == nuc.coords).all(-1).any(-1)==True).any()):\n",
    "                summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['exon'].append(exon_id)\n",
    "                \n",
    "        ## Check for colocalization between exonic and intronic spots (nascent mRNA)\n",
    "        for intron in regionprops(red,intensity_image=sitk.GetArrayFromImage(intron_image)):\n",
    "            intron_id=intron.label\n",
    "            if (((intron.coords[:, None] == nuc.coords).all(-1).any(-1)==True).all()):\n",
    "                summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['intron'].append(intron_id)         \n",
    "\n",
    "                # Check for colocalization between exonic and intronic spots\n",
    "                for nucexon in summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['exon']:\n",
    "                    if (((intron.coords[:, None] == regionprops(green)[nucexon-1].coords).all(-1).any(-1)==True).any()):\n",
    "                        summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['colocspot'].append([intron_id,nucexon])\n",
    "                        \n",
    "        ## Finally, on some locations, one intronic spot has colocation with two (or more) exonic spots or\n",
    "        ## one exonic spots has colocation with two (or more) intronic spots.\n",
    "        ## we are going to set them all equally and consider them just as \"one nascent mRNA\" or \"one burst\"\n",
    "        ## Handle cases where colocalization involves multiple exonic or intronic spots\n",
    "        X=np.array(summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['colocspot'])\n",
    "        W= np.array(summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['intron'])\n",
    "        Y=np.array(summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['exon'])\n",
    "\n",
    "        if(len(X)>0):\n",
    "            # Check if there are overlapping spots and adjust to count them as one nascent mRNA\n",
    "            if((len(X[:,0])>len(np.unique(X[:,0]))) or(len(X[:,1])>len(np.unique(X[:,1])))) :\n",
    "                unique, counts = np.unique(X[:,0], return_counts=True)\n",
    "                unique2, counts2 = np.unique(X[:,1], return_counts=True)\n",
    "                if((len(X[:,0])>len(np.unique(X[:,0])))):\n",
    "                    for i in (unique[(counts)>1]):\n",
    "                        # Merge non-overlapping and overlapping spots into the nascent array\n",
    "                        summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['nascent'].append([i,X[X[:,0]==i][:,1]])\n",
    "                    v=([X[X[:,0]==i][0][0] for i in unique[(counts)==1]])\n",
    "                    u=([X[X[:,0]==i][0][1] for i in unique[(counts)==1]])\n",
    "                    r=([X[X[:,0]==i][0] for i in unique[(counts)==1]])\n",
    "                    if (len(u)>len(np.unique(u))):\n",
    "                        for j in np.unique(u):\n",
    "                            summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['nascent'].append([X[X[:,1]==j][:,0],j])\n",
    "                    elif(len(r)>0):\n",
    "                        summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['nascent']=np.vstack((summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['nascent'],r))\n",
    "                else:\n",
    "                    # Handle overlapping exonic spots\n",
    "                    for k in (unique2):\n",
    "                        summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['nascent'].append([X[X[:,1]==k][:,0],k]) \n",
    "            else:\n",
    "                # If there are no overlaps, nascent mRNA is the same as colocalized spots\n",
    "                summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['nascent']=summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['colocspot']\n",
    "        else:\n",
    "            # If no nascent spots are found, store the count of nascent mRNAs\n",
    "            summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['Nnascent']=len(summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['nascent'])   \n",
    "    ## Save the segmentation summary as a pickle file\n",
    "    with open(os.path.join(seg_dir, 'summary_t_'+str(t)+'_f_'+str(f)+'.pickle'), 'wb') as handle:\n",
    "        pickle.dump(summary, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 34455.50477766991\n"
     ]
    }
   ],
   "source": [
    "fpath = raw_img_dir_leave_top_bottom\n",
    "from time import time\n",
    "begin=time()\n",
    "\n",
    "Parallel(n_jobs=2)(delayed(image_segmentation)(filename,newfeatures) for filename in newfeatures['filename'].unique())\n",
    "        \n",
    "print(\"Total time:\",time()-begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
